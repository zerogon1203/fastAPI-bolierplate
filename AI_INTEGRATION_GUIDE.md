# AI í†µí•© ê°€ì´ë“œ (Langchain + MCP)

## ê°œìš”

ì´ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ëŠ” AI ì‹œëŒ€ë¥¼ ëŒ€ë¹„í•˜ì—¬ Langchainê³¼ MCP(Model Context Protocol)ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.

## ê¸°ëŠ¥

### ğŸ”— Langchain í†µí•©
- **ë‹¤ì¤‘ LLM í”„ë¡œë°”ì´ë” ì§€ì›**: OpenAI, Anthropic, Google ë“±
- **LangServe**: FastAPIì™€ Langchain ì²´ì¸ í†µí•©
- **RAG (Retrieval-Augmented Generation)**: ë¬¸ì„œ ê¸°ë°˜ QA ì‹œìŠ¤í…œ
- **AI ì—ì´ì „íŠ¸**: ìë™í™”ëœ ì‘ì—… ìˆ˜í–‰
- **ë²¡í„° ê²€ìƒ‰**: ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰

### ğŸ¤– MCP (Model Context Protocol) ì§€ì›
- **í‘œì¤€ í”„ë¡œí† ì½œ**: AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°„ ìƒí˜¸ìš´ìš©ì„±
- **ë‹¤ì–‘í•œ ì„œë²„**: íŒŒì¼ì‹œìŠ¤í…œ, GitHub, Slack ë“± ì—°ë™
- **ì»¤ìŠ¤í…€ ì„œë²„**: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ MCP ì„œë²„ë¡œ ë…¸ì¶œ
- **ì•ˆì „í•œ í†µì‹ **: í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ê°„ ì•ˆì „í•œ ë°ì´í„° êµí™˜

## ì„¤ì • ë°©ë²•

### 1. API í‚¤ ì„¤ì •

`.env` íŒŒì¼ì— í•„ìš”í•œ API í‚¤ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”:

```bash
# í•„ìˆ˜ LLM API í‚¤ (í•˜ë‚˜ ì´ìƒ)
OPENAI_API_KEY=your-openai-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Langchain ëª¨ë‹ˆí„°ë§ (ì„ íƒì‚¬í•­)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-api-key
LANGCHAIN_PROJECT=fastapi-boilerplate

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (ì„ íƒì‚¬í•­)
PINECONE_API_KEY=your-pinecone-api-key
WEAVIATE_URL=http://localhost:8080
```

### 2. MCP ì„œë²„ ì„¤ì •

`config/mcp_servers.json` íŒŒì¼ì—ì„œ ì‚¬ìš©í•  MCP ì„œë²„ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”.

### 3. AI ì´ˆê¸° ì„¤ì •

```bash
make ai-setup
```

## ì‚¬ìš© ì˜ˆì‹œ

### Langchain ì²´ì¸ ì˜ˆì‹œ

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from app.ai.providers.openai_provider import get_openai_llm

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """
ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
ì§ˆë¬¸: {question}
ë‹µë³€:
"""

prompt = PromptTemplate(
    input_variables=["question"],
    template=template
)

# LLM ì²´ì¸ ìƒì„±
llm = get_openai_llm()
chain = LLMChain(llm=llm, prompt=prompt)

# ì‹¤í–‰
result = chain.run(question="FastAPIì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
```

### MCP ì„œë²„ êµ¬í˜„ ì˜ˆì‹œ

```python
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp import Context

# MCP ì„œë²„ ìƒì„±
mcp = FastMCP("database-tools")

@mcp.tool()
async def get_user_count(ctx: Context) -> str:
    """ì‚¬ìš©ì ìˆ˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤"""
    # ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë¡œì§
    count = await get_user_count_from_db()
    return f"ì „ì²´ ì‚¬ìš©ì ìˆ˜: {count}ëª…"

@mcp.resource("users://{user_id}")
def get_user_profile(user_id: str) -> str:
    """ì‚¬ìš©ì í”„ë¡œí•„ì„ ì¡°íšŒí•©ë‹ˆë‹¤"""
    # ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ë¡œì§
    profile = get_user_profile_from_db(user_id)
    return f"ì‚¬ìš©ì ì •ë³´: {profile}"
```

### RAG ì‹œìŠ¤í…œ ì˜ˆì‹œ

```python
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from app.ai.providers.openai_provider import get_openai_llm

# ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_documents(
    documents, 
    embeddings, 
    index_name="knowledge-base"
)

# RAG ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=get_openai_llm(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# ì§ˆë¬¸ ë‹µë³€
answer = qa_chain.run("íšŒì‚¬ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”")
```

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
app/ai/
â”œâ”€â”€ chains/              # Langchain ì²´ì¸ë“¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ qa_chain.py     # Q&A ì²´ì¸
â”‚   â”œâ”€â”€ summary_chain.py # ìš”ì•½ ì²´ì¸
â”‚   â””â”€â”€ analysis_chain.py # ë¶„ì„ ì²´ì¸
â”œâ”€â”€ agents/              # AI ì—ì´ì „íŠ¸ë“¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_agent.py   # ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸
â”‚   â””â”€â”€ support_agent.py # ê³ ê° ì§€ì› ì—ì´ì „íŠ¸
â”œâ”€â”€ tools/               # AI ë„êµ¬ë“¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database_tools.py # ë°ì´í„°ë² ì´ìŠ¤ ë„êµ¬
â”‚   â””â”€â”€ api_tools.py    # API í˜¸ì¶œ ë„êµ¬
â”œâ”€â”€ prompts/             # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ qa_prompts.py   # Q&A í”„ë¡¬í”„íŠ¸
â”‚   â””â”€â”€ analysis_prompts.py # ë¶„ì„ í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ embeddings/          # ì„ë² ë”© ê´€ë ¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ vector_store.py # ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬
â”œâ”€â”€ memory/              # ëŒ€í™” ë©”ëª¨ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_memory.py  # ì±„íŒ… ë©”ëª¨ë¦¬ ê´€ë¦¬
â”œâ”€â”€ mcp/                 # MCP ì„œë²„/í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ servers/         # MCP ì„œë²„ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database_server.py # ë°ì´í„°ë² ì´ìŠ¤ MCP ì„œë²„
â”‚   â”‚   â””â”€â”€ custom_server.py   # ì»¤ìŠ¤í…€ MCP ì„œë²„
â”‚   â”œâ”€â”€ clients/         # MCP í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ mcp_client.py # MCP í´ë¼ì´ì–¸íŠ¸ ë˜í¼
â”‚   â””â”€â”€ tools/           # MCP ë„êµ¬ë“¤
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ mcp_tools.py # MCP ë„êµ¬ ì •ì˜
â””â”€â”€ providers/           # LLM í”„ë¡œë°”ì´ë” ì„¤ì •
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ openai_provider.py    # OpenAI ì„¤ì •
    â”œâ”€â”€ anthropic_provider.py # Anthropic ì„¤ì •
    â””â”€â”€ base_provider.py      # ê¸°ë³¸ í”„ë¡œë°”ì´ë” ì¸í„°í˜ì´ìŠ¤
```

## API ì—”ë“œí¬ì¸íŠ¸ ì˜ˆì‹œ

### AI ì±„íŒ… API

```python
@router.post("/ai/chat")
async def chat_with_ai(request: ChatRequest):
    """AIì™€ ì±„íŒ…í•©ë‹ˆë‹¤"""
    chain = get_chat_chain()
    response = await chain.arun(
        message=request.message,
        history=request.history
    )
    return {"response": response}
```

### ë¬¸ì„œ ë¶„ì„ API

```python
@router.post("/ai/analyze-document")
async def analyze_document(file: UploadFile):
    """ë¬¸ì„œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤"""
    # íŒŒì¼ ì²˜ë¦¬
    text = extract_text_from_file(file)
    
    # AI ë¶„ì„
    analysis_chain = get_analysis_chain()
    result = await analysis_chain.arun(text=text)
    
    return {"analysis": result}
```

### RAG ê²€ìƒ‰ API

```python
@router.post("/ai/search")
async def search_knowledge(query: SearchQuery):
    """ì§€ì‹ ê¸°ë°˜ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    qa_chain = get_qa_chain()
    answer = await qa_chain.arun(question=query.question)
    return {"answer": answer}
```

## ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

### LangSmith í†µí•©
- Langchain ì‹¤í–‰ ì¶”ì 
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ë””ë²„ê¹… ì§€ì›

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„°ë§

## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### ì…ë ¥ ê²€ì¦
- í”„ë¡¬í”„íŠ¸ ì£¼ì… ë°©ì§€
- ì…ë ¥ ë°ì´í„° sanitization
- Rate limiting ì ìš©

### API í‚¤ ë³´ì•ˆ
- í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- í‚¤ ë¡œí…Œì´ì…˜ ì •ì±…
- ìµœì†Œ ê¶Œí•œ ì›ì¹™

## í™•ì¥ ê°€ëŠ¥ì„±

### ìƒˆë¡œìš´ LLM í”„ë¡œë°”ì´ë” ì¶”ê°€
1. `app/ai/providers/` ì— ìƒˆ í”„ë¡œë°”ì´ë” ì¶”ê°€
2. ê¸°ë³¸ í”„ë¡œë°”ì´ë” ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
3. í™˜ê²½ ì„¤ì • ì¶”ê°€

### ì»¤ìŠ¤í…€ MCP ì„œë²„ ê°œë°œ
1. `app/ai/mcp/servers/` ì— ìƒˆ ì„œë²„ ì¶”ê°€
2. MCP í”„ë¡œí† ì½œ êµ¬í˜„
3. ì„¤ì • íŒŒì¼ì— ì„œë²„ ë“±ë¡

### AI ê¸°ëŠ¥ í™•ì¥
1. ìƒˆë¡œìš´ ì²´ì¸ì´ë‚˜ ì—ì´ì „íŠ¸ ê°œë°œ
2. ì»¤ìŠ¤í…€ ë„êµ¬ ì¶”ê°€
3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™•ì¥

## í…ŒìŠ¤íŠ¸

```bash
# AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
make ai-test

# MCP ì„œë²„ í…ŒìŠ¤íŠ¸
make mcp-test

# Langchain ì„œë²„ ì‹¤í–‰
make langchain-serve
```

## ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

1. **API í‚¤ ì˜¤ë¥˜**: `.env` íŒŒì¼ì˜ API í‚¤ í™•ì¸
2. **MCP ì—°ê²° ì‹¤íŒ¨**: MCP ì„œë²„ ì„¤ì • ë° ë„¤íŠ¸ì›Œí¬ í™•ì¸
3. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: í° ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ì²­í¬ í¬ê¸° ì¡°ì •
4. **í† í° í•œë„ ì´ˆê³¼**: í”„ë¡¬í”„íŠ¸ í¬ê¸° ìµœì í™”

### ë¡œê·¸ í™•ì¸

```bash
# AI ê´€ë ¨ ë¡œê·¸ í™•ì¸
tail -f logs/ai.log

# MCP ì„œë²„ ë¡œê·¸ í™•ì¸
tail -f logs/mcp.log
```

ì´ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ AI ê¸°ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”! ğŸš€ 